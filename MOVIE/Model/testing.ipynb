{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import threading\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    \"\"\"\n",
    "    Clase para representar un usuario con sus ratings y embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, user_id: str):\n",
    "        self.id = user_id\n",
    "        self.ratings: List[Tuple[int, float]] = []\n",
    "        self.embedding: Optional[np.ndarray] = None\n",
    "    \n",
    "    def add_rating(self, item_id: int, rating: float):\n",
    "        \"\"\"Agregar un rating del usuario\"\"\"\n",
    "        if not pd.isna(rating):  # Solo agregar ratings válidos\n",
    "            self.ratings.append((item_id, rating))\n",
    "    \n",
    "    def get_ratings_vector(self, total_items: int) -> np.ndarray:\n",
    "        \"\"\"Obtener vector de ratings completo (con 0s para items no calificados)\"\"\"\n",
    "        vector = np.zeros(total_items)\n",
    "        for item_id, rating in self.ratings:\n",
    "            vector[item_id] = rating\n",
    "        return vector\n",
    "    \n",
    "    def get_ratings_dict(self) -> dict:\n",
    "        \"\"\"Obtener ratings como diccionario\"\"\"\n",
    "        return {item_id: rating for item_id, rating in self.ratings}\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convertir a diccionario para serialización\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'ratings': self.ratings,\n",
    "            'embedding': self.embedding.tolist() if self.embedding is not None else None\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict):\n",
    "        \"\"\"Crear User desde diccionario\"\"\"\n",
    "        user = cls(data['id'])\n",
    "        user.ratings = data['ratings']\n",
    "        user.embedding = np.array(data['embedding']) if data['embedding'] is not None else None\n",
    "        return user\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"User {self.id}: {len(self.ratings)} ratings, embedding: {self.embedding is not None}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6233db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    \"\"\"\n",
    "    Clase para representar un chunk de usuarios con su embedding representativo\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_id: int):\n",
    "        self.id = chunk_id\n",
    "        self.users: List[User] = []\n",
    "        self.embedding: Optional[np.ndarray] = None\n",
    "        self.centroid: Optional[np.ndarray] = None\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def add_user(self, user: User):\n",
    "        \"\"\"Agregar usuario al chunk de forma thread-safe y actualizar embedding promedio\"\"\"\n",
    "        with self._lock:\n",
    "            self.users.append(user)\n",
    "            self._update_average_embedding()\n",
    "    \n",
    "    def _update_average_embedding(self):\n",
    "        \"\"\"Actualizar embedding promedio del chunk\"\"\"\n",
    "        if not self.users or self.users[0].embedding is None:\n",
    "            self.embedding = None\n",
    "            return\n",
    "        \n",
    "        # Calcular embedding promedio de todos los usuarios\n",
    "        embeddings = np.array([user.embedding for user in self.users])\n",
    "        self.embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    def calculate_representative_embedding(self):\n",
    "        \"\"\"Calcular embedding representativo del chunk (centroide)\"\"\"\n",
    "        self._update_average_embedding()\n",
    "        if self.embedding is not None:\n",
    "            self.centroid = self.embedding.copy()\n",
    "    \n",
    "    def cosine_distance(self, user_embedding: np.ndarray) -> float:\n",
    "        \"\"\"Calcular distancia coseno entre embedding promedio del chunk y usuario\"\"\"\n",
    "        if self.embedding is None:\n",
    "            return float('inf')  # Distancia infinita si no hay embedding\n",
    "        \n",
    "        dot_product = np.dot(self.embedding, user_embedding)\n",
    "        norm_chunk = np.linalg.norm(self.embedding)\n",
    "        norm_user = np.linalg.norm(user_embedding)\n",
    "        \n",
    "        if norm_chunk == 0 or norm_user == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        # Similitud coseno\n",
    "        cosine_similarity = dot_product / (norm_chunk * norm_user)\n",
    "        \n",
    "        # Distancia coseno = 1 - similitud coseno\n",
    "        return 1.0 - cosine_similarity\n",
    "    \n",
    "    def cosine_similarity(self, user_embedding: np.ndarray) -> float:\n",
    "        \"\"\"Calcular similitud coseno entre embedding promedio del chunk y usuario\"\"\"\n",
    "        distance = self.cosine_distance(user_embedding)\n",
    "        if distance == float('inf'):\n",
    "            return 0.0\n",
    "        return 1.0 - distance\n",
    "    \n",
    "    def get_user_count(self) -> int:\n",
    "        \"\"\"Obtener número de usuarios en el chunk\"\"\"\n",
    "        return len(self.users)\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convertir a diccionario para serialización\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'users': [user.to_dict() for user in self.users],\n",
    "            'embedding': self.embedding.tolist() if self.embedding is not None else None,\n",
    "            'centroid': self.centroid.tolist() if self.centroid is not None else None\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict):\n",
    "        \"\"\"Crear Chunk desde diccionario\"\"\"\n",
    "        chunk = cls(data['id'])\n",
    "        chunk.users = [User.from_dict(user_data) for user_data in data['users']]\n",
    "        chunk.embedding = np.array(data['embedding']) if data['embedding'] is not None else None\n",
    "        chunk.centroid = np.array(data['centroid']) if data['centroid'] is not None else None\n",
    "        return chunk\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Chunk {self.id}: {len(self.users)} users, embedding: {self.embedding is not None}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071beb6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMovieRatingsProcessor\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Procesador principal para convertir dataset de ratings en chunks de usuarios\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, num_chunks: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, output_folder: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks_data\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mMovieRatingsProcessor\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInicializado con \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_chunks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks, umbral: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m threads para procesamiento\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_process_chunk_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, chunk_data: \u001b[43mTuple\u001b[49m[pd\u001b[38;5;241m.\u001b[39mDataFrame, \u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[User]:\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Procesar un chunk de datos en un thread separado\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     df_chunk, chunk_index \u001b[38;5;241m=\u001b[39m chunk_data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "class MovieRatingsProcessor:\n",
    "    \"\"\"\n",
    "    Procesador principal para convertir dataset de ratings en chunks de usuarios\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5, num_chunks: int = 4, output_folder: str = \"chunks_data\"):\n",
    "        self.threshold = threshold\n",
    "        self.num_chunks = num_chunks\n",
    "        self.output_folder = output_folder\n",
    "        self.chunks: List[Chunk] = []\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = None\n",
    "        self.total_items = 0\n",
    "        self.num_threads = max(1, multiprocessing.cpu_count() // 2)\n",
    "        \n",
    "        # Crear carpeta de salida si no existe\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "        \n",
    "        # Inicializar chunks\n",
    "        for i in range(self.num_chunks):\n",
    "            self.chunks.append(Chunk(i))\n",
    "            \n",
    "        print(f\"Inicializado con {self.num_chunks} chunks, umbral: {threshold}\")\n",
    "        print(f\"Usando {self.num_threads} threads para procesamiento\")\n",
    "    \n",
    "    def _process_chunk_data(self, chunk_data: Tuple[pd.DataFrame, int]) -> List[User]:\n",
    "        \"\"\"\n",
    "        Procesar un chunk de datos en un thread separado\n",
    "        \"\"\"\n",
    "        df_chunk, chunk_index = chunk_data\n",
    "        users = []\n",
    "        \n",
    "        # Obtener columnas de usuarios (todas excepto la primera)\n",
    "        user_columns = df_chunk.columns[1:]\n",
    "        \n",
    "        # Crear usuarios para este chunk\n",
    "        for user_id in user_columns:\n",
    "            user = User(str(user_id))\n",
    "            \n",
    "            # Procesar ratings de este usuario\n",
    "            for idx, row in df_chunk.iterrows():\n",
    "                item_id = int(row['item_id'])\n",
    "                rating = row[user_id]\n",
    "                \n",
    "                if pd.notna(rating):  # Solo agregar ratings válidos\n",
    "                    user.add_rating(item_id, float(rating))\n",
    "            \n",
    "            if user.ratings:  # Solo agregar usuarios con ratings\n",
    "                users.append(user)\n",
    "        \n",
    "        return users\n",
    "    \n",
    "    def load_from_npz(self, npz_path: str, movies_mapping_path: str = None, \n",
    "                     users_mapping_path: str = None, metadata_path: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Cargar dataset desde archivos NPZ dispersos\n",
    "        \n",
    "        Args:\n",
    "            npz_path: Ruta al archivo .npz con la matriz dispersa\n",
    "            movies_mapping_path: Ruta al archivo .pkl con mapeo de películas (opcional)\n",
    "            users_mapping_path: Ruta al archivo .pkl con mapeo de usuarios (opcional)\n",
    "            metadata_path: Ruta al archivo .pkl con metadatos (opcional)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from scipy.sparse import load_npz\n",
    "            \n",
    "            print(f\"Cargando matriz dispersa desde: {npz_path}\")\n",
    "            sparse_matrix = load_npz(npz_path)\n",
    "            \n",
    "            print(f\"Matriz cargada: {sparse_matrix.shape}\")\n",
    "            print(f\"Densidad de la matriz: {sparse_matrix.nnz / (sparse_matrix.shape[0] * sparse_matrix.shape[1]):.4f}\")\n",
    "            \n",
    "            # Cargar mapeos si están disponibles\n",
    "            movies_mapping = None\n",
    "            users_mapping = None\n",
    "            metadata = None\n",
    "            \n",
    "            if movies_mapping_path and os.path.exists(movies_mapping_path):\n",
    "                with open(movies_mapping_path, 'rb') as f:\n",
    "                    movies_mapping = pickle.load(f)\n",
    "                print(f\"Mapeo de películas cargado: {len(movies_mapping)} películas\")\n",
    "            \n",
    "            if users_mapping_path and os.path.exists(users_mapping_path):\n",
    "                with open(users_mapping_path, 'rb') as f:\n",
    "                    users_mapping = pickle.load(f)\n",
    "                print(f\"Mapeo de usuarios cargado: {len(users_mapping)} usuarios\")\n",
    "            \n",
    "            if metadata_path and os.path.exists(metadata_path):\n",
    "                with open(metadata_path, 'rb') as f:\n",
    "                    metadata = pickle.load(f)\n",
    "                print(f\"Metadatos cargados: {metadata}\")\n",
    "            \n",
    "            # Convertir matriz dispersa a formato denso para procesamiento\n",
    "            # Nota: Para matrices muy grandes, considerar procesamiento por lotes\n",
    "            dense_matrix = sparse_matrix.toarray()\n",
    "            \n",
    "            # La matriz tiene forma (usuarios, películas)\n",
    "            n_users, n_movies = dense_matrix.shape\n",
    "            self.total_items = n_movies\n",
    "            \n",
    "            print(f\"Procesando {n_users} usuarios y {n_movies} películas\")\n",
    "            \n",
    "            # Crear usuarios desde la matriz\n",
    "            all_users = self._create_users_from_matrix(dense_matrix, users_mapping)\n",
    "            \n",
    "            print(f\"Creados {len(all_users)} usuarios\")\n",
    "            \n",
    "            # Generar embeddings para todos los usuarios\n",
    "            self._generate_user_embeddings_from_matrix(dense_matrix, all_users)\n",
    "            \n",
    "            # Distribuir usuarios en chunks basado en similitud\n",
    "            self._distribute_users_to_chunks(all_users)\n",
    "            \n",
    "            # Calcular embeddings representativos para cada chunk\n",
    "            self._calculate_chunk_embeddings()\n",
    "            \n",
    "            # Guardar chunks en archivos\n",
    "            self._save_chunks_to_files()\n",
    "            \n",
    "            # Limpiar memoria\n",
    "            self._clear_memory()\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"Error: scipy no está instalado. Ejecute: pip install scipy\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar archivo NPZ: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_users_from_matrix(self, matrix: np.ndarray, users_mapping: dict = None) -> List[User]:\n",
    "        \"\"\"\n",
    "        Crear usuarios desde matriz densa\n",
    "        \n",
    "        Args:\n",
    "            matrix: Matriz densa (usuarios x películas)\n",
    "            users_mapping: Mapeo de índices a IDs de usuario\n",
    "        \"\"\"\n",
    "        users = []\n",
    "        n_users, n_movies = matrix.shape\n",
    "        \n",
    "        print(\"Creando usuarios desde matriz...\")\n",
    "        \n",
    "        for user_idx in range(n_users):\n",
    "            # Obtener ID del usuario\n",
    "            if users_mapping:\n",
    "                user_id = users_mapping.get(user_idx, str(user_idx))\n",
    "            else:\n",
    "                user_id = str(user_idx)\n",
    "            \n",
    "            user = User(user_id)\n",
    "            \n",
    "            # Extraer ratings no cero para este usuario\n",
    "            user_ratings = matrix[user_idx, :]\n",
    "            non_zero_indices = np.nonzero(user_ratings)[0]\n",
    "            \n",
    "            # Agregar ratings al usuario\n",
    "            for movie_idx in non_zero_indices:\n",
    "                rating = user_ratings[movie_idx]\n",
    "                if rating != 0:  # Verificar que no sea cero\n",
    "                    user.add_rating(movie_idx, float(rating))\n",
    "            \n",
    "            if user.ratings:  # Solo agregar usuarios con ratings\n",
    "                users.append(user)\n",
    "            \n",
    "            # Mostrar progreso cada 1000 usuarios\n",
    "            if (user_idx + 1) % 1000 == 0:\n",
    "                print(f\"Procesados {user_idx + 1}/{n_users} usuarios\")\n",
    "        \n",
    "        return users\n",
    "    \n",
    "    def _generate_user_embeddings_from_matrix(self, matrix: np.ndarray, users: List[User]):\n",
    "        \"\"\"\n",
    "        Generar embeddings para usuarios usando matriz densa directamente\n",
    "        \"\"\"\n",
    "        print(\"Generando embeddings desde matriz...\")\n",
    "        \n",
    "        # Usar la matriz directamente (ya está en formato usuario x película)\n",
    "        ratings_matrix = matrix.copy()\n",
    "        \n",
    "        # Filtrar solo las filas de usuarios que tienen ratings\n",
    "        valid_user_indices = []\n",
    "        for i, user in enumerate(users):\n",
    "            if user.ratings:\n",
    "                valid_user_indices.append(i)\n",
    "        \n",
    "        # Tomar solo las filas de usuarios válidos\n",
    "        if valid_user_indices:\n",
    "            ratings_matrix = ratings_matrix[valid_user_indices, :]\n",
    "        \n",
    "        print(f\"Matriz de ratings para embeddings: {ratings_matrix.shape}\")\n",
    "        \n",
    "        # Normalizar y aplicar PCA\n",
    "        ratings_matrix = self.scaler.fit_transform(ratings_matrix)\n",
    "        \n",
    "        n_components = min(50, ratings_matrix.shape[1], ratings_matrix.shape[0])\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        embeddings = self.pca.fit_transform(ratings_matrix)\n",
    "        \n",
    "        # Asignar embeddings a usuarios\n",
    "        for i, user in enumerate(users):\n",
    "            user.embedding = embeddings[i]\n",
    "        \n",
    "        print(f\"Embeddings generados: {embeddings.shape}\")\n",
    "        print(f\"Varianza explicada: {self.pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    \n",
    "    def load_from_csv(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Cargar dataset desde archivo CSV usando multithreading\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Cargando datos desde: {filepath}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            # Verificar que la primera columna sea item_id\n",
    "            if df.columns[0] != 'item_id':\n",
    "                print(\"Advertencia: Se esperaba 'item_id' como primera columna\")\n",
    "            \n",
    "            self.total_items = len(df)\n",
    "            \n",
    "            # Dividir DataFrame en chunks para procesamiento paralelo\n",
    "            chunk_size = max(1, len(df) // self.num_threads)\n",
    "            df_chunks = []\n",
    "            \n",
    "            for i in range(0, len(df), chunk_size):\n",
    "                chunk_df = df.iloc[i:i + chunk_size].copy()\n",
    "                df_chunks.append((chunk_df, i // chunk_size))\n",
    "            \n",
    "            print(f\"Dividiendo datos en {len(df_chunks)} chunks para procesamiento\")\n",
    "            \n",
    "            # Procesar chunks en paralelo\n",
    "            all_users = []\n",
    "            with ThreadPoolExecutor(max_workers=self.num_threads) as executor:\n",
    "                future_to_chunk = {executor.submit(self._process_chunk_data, chunk_data): chunk_data \n",
    "                                 for chunk_data in df_chunks}\n",
    "                \n",
    "                for future in as_completed(future_to_chunk):\n",
    "                    chunk_users = future.result()\n",
    "                    all_users.extend(chunk_users)\n",
    "            \n",
    "            print(f\"Procesados {len(all_users)} usuarios en total\")\n",
    "            \n",
    "            # Generar embeddings para todos los usuarios\n",
    "            self._generate_user_embeddings(all_users)\n",
    "            \n",
    "            # Distribuir usuarios en chunks basado en similitud\n",
    "            self._distribute_users_to_chunks(all_users)\n",
    "            \n",
    "            # Calcular embeddings representativos para cada chunk\n",
    "            self._calculate_chunk_embeddings()\n",
    "            \n",
    "            # Guardar chunks en archivos\n",
    "            self._save_chunks_to_files()\n",
    "            \n",
    "            # Limpiar memoria\n",
    "            self._clear_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar archivo: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _generate_user_embeddings(self, users: List[User]):\n",
    "        \"\"\"\n",
    "        Generar embeddings para todos los usuarios\n",
    "        \"\"\"\n",
    "        print(\"Generando embeddings para usuarios...\")\n",
    "        \n",
    "        # Encontrar el rango real de item_ids\n",
    "        all_item_ids = set()\n",
    "        for user in users:\n",
    "            for item_id, rating in user.ratings:\n",
    "                all_item_ids.add(item_id)\n",
    "        \n",
    "        if not all_item_ids:\n",
    "            print(\"No se encontraron ratings válidos\")\n",
    "            return\n",
    "        \n",
    "        min_item_id = min(all_item_ids)\n",
    "        max_item_id = max(all_item_ids)\n",
    "        \n",
    "        print(f\"Rango de item_ids: {min_item_id} - {max_item_id}\")\n",
    "        print(f\"Total items únicos: {len(all_item_ids)}\")\n",
    "        \n",
    "        # Crear mapeo de item_id a índice\n",
    "        self.item_id_to_index = {item_id: idx for idx, item_id in enumerate(sorted(all_item_ids))}\n",
    "        self.index_to_item_id = {idx: item_id for item_id, idx in self.item_id_to_index.items()}\n",
    "        \n",
    "        # Actualizar total_items al número real de items únicos\n",
    "        self.total_items = len(all_item_ids)\n",
    "        \n",
    "        # Crear matriz de ratings usando el mapeo\n",
    "        ratings_matrix = np.zeros((len(users), self.total_items))\n",
    "        \n",
    "        for i, user in enumerate(users):\n",
    "            for item_id, rating in user.ratings:\n",
    "                if item_id in self.item_id_to_index:\n",
    "                    matrix_index = self.item_id_to_index[item_id]\n",
    "                    ratings_matrix[i, matrix_index] = rating\n",
    "        \n",
    "        print(f\"Matriz de ratings creada: {ratings_matrix.shape}\")\n",
    "        \n",
    "        # Normalizar y aplicar PCA\n",
    "        ratings_matrix = self.scaler.fit_transform(ratings_matrix)\n",
    "        \n",
    "        n_components = min(50, ratings_matrix.shape[1], ratings_matrix.shape[0])\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        embeddings = self.pca.fit_transform(ratings_matrix)\n",
    "        \n",
    "        # Asignar embeddings a usuarios\n",
    "        for i, user in enumerate(users):\n",
    "            user.embedding = embeddings[i]\n",
    "        \n",
    "        print(f\"Embeddings generados: {embeddings.shape}\")\n",
    "        #print(f\"Varianza explicada:\n",
    "    \n",
    "    def _distribute_users_to_chunks(self, users: List[User]):\n",
    "        \"\"\"\n",
    "        Distribuir usuarios en chunks basado en distancia coseno\n",
    "        \"\"\"\n",
    "        print(\"Distribuyendo usuarios en chunks...\")\n",
    "        \n",
    "        # Inicializar primer chunk con el primer usuario\n",
    "        if users:\n",
    "            self.chunks[0].add_user(users[0])\n",
    "            print(f\"Usuario {users[0].id} agregado al Chunk 0 (inicialización)\")\n",
    "        \n",
    "        # Distribuir el resto de usuarios\n",
    "        for user_idx, user in enumerate(users[1:], 1):\n",
    "            user_assigned = False\n",
    "            \n",
    "            # Probar cada chunk secuencialmente\n",
    "            for chunk_idx, chunk in enumerate(self.chunks):\n",
    "                # Si el chunk está vacío, agregamos el usuario\n",
    "                if len(chunk.users) == 0:\n",
    "                    chunk.add_user(user)\n",
    "                    user_assigned = True\n",
    "                    print(f\"Usuario {user.id} agregado al Chunk {chunk_idx} (chunk vacío)\")\n",
    "                    break\n",
    "                \n",
    "                # Calcular distancia coseno\n",
    "                cosine_distance = chunk.cosine_distance(user.embedding)\n",
    "                \n",
    "                print(f\"Usuario {user.id} vs Chunk {chunk_idx}: distancia coseno = {cosine_distance:.4f}\")\n",
    "                \n",
    "                # Si la distancia es menor que el umbral, agregar al chunk\n",
    "                if cosine_distance < self.threshold:\n",
    "                    chunk.add_user(user)\n",
    "                    user_assigned = True\n",
    "                    print(f\"Usuario {user.id} agregado al Chunk {chunk_idx} (distancia < {self.threshold})\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Usuario {user.id} no agregado al Chunk {chunk_idx} (distancia >= {self.threshold})\")\n",
    "            \n",
    "            # Si no se pudo asignar a ningún chunk, agregar al último chunk\n",
    "            if not user_assigned:\n",
    "                last_chunk = self.chunks[-1]\n",
    "                last_chunk.add_user(user)\n",
    "                print(f\"Usuario {user.id} agregado al Chunk {len(self.chunks)-1} (último chunk por defecto)\")\n",
    "            \n",
    "            # Mostrar progreso cada 10 usuarios\n",
    "            if user_idx % 10 == 0:\n",
    "                print(f\"Procesados {user_idx}/{len(users)-1} usuarios\")\n",
    "        \n",
    "        # Mostrar estadísticas finales de distribución\n",
    "        print(\"\\nDistribución final de usuarios:\")\n",
    "        for chunk in self.chunks:\n",
    "            print(f\"Chunk {chunk.id}: {len(chunk.users)} usuarios\")\n",
    "            if len(chunk.users) > 0:\n",
    "                print(f\"  Embedding promedio shape: {chunk.embedding.shape if chunk.embedding is not None else 'None'}\")\n",
    "        \n",
    "        # Verificar que todos los usuarios fueron asignados\n",
    "        total_assigned = sum(len(chunk.users) for chunk in self.chunks)\n",
    "        print(f\"\\nTotal usuarios asignados: {total_assigned}/{len(users)}\")\n",
    "        \n",
    "        if total_assigned != len(users):\n",
    "            print(\"¡ADVERTENCIA! No todos los usuarios fueron asignados correctamente\")\n",
    "    \n",
    "    def _calculate_chunk_embeddings(self):\n",
    "        \"\"\"\n",
    "        Calcular embeddings representativos para cada chunk\n",
    "        \"\"\"\n",
    "        print(\"Calculando embeddings representativos de chunks...\")\n",
    "        \n",
    "        for chunk in self.chunks:\n",
    "            chunk.calculate_representative_embedding()\n",
    "    \n",
    "    def _save_chunks_to_files(self):\n",
    "        \"\"\"\n",
    "        Guardar cada chunk en un archivo individual\n",
    "        \"\"\"\n",
    "        print(\"Guardando chunks en archivos...\")\n",
    "        \n",
    "        for chunk in self.chunks:\n",
    "            filename = os.path.join(self.output_folder, f\"chunk_{chunk.id}.pkl\")\n",
    "            \n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(chunk.to_dict(), f)\n",
    "            \n",
    "            print(f\"Chunk {chunk.id} guardado en: {filename}\")\n",
    "        \n",
    "        # Guardar metadatos del procesador\n",
    "        metadata = {\n",
    "            'threshold': self.threshold,\n",
    "            'num_chunks': self.num_chunks,\n",
    "            'total_items': self.total_items,\n",
    "            'scaler': self.scaler,\n",
    "            'pca': self.pca\n",
    "        }\n",
    "        \n",
    "        metadata_file = os.path.join(self.output_folder, \"processor_metadata.pkl\")\n",
    "        with open(metadata_file, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"Metadatos guardados en: {metadata_file}\")\n",
    "    \n",
    "    def _clear_memory(self):\n",
    "        \"\"\"\n",
    "        Limpiar datos de memoria, conservando solo la estructura del procesador\n",
    "        \"\"\"\n",
    "        print(\"Limpiando memoria...\")\n",
    "        \n",
    "        # Limpiar chunks de memoria\n",
    "        for chunk in self.chunks:\n",
    "            chunk.users.clear()\n",
    "            chunk.embedding = None\n",
    "            chunk.centroid = None\n",
    "        \n",
    "        print(\"Memoria limpiada. Solo se conservan archivos y estructura del procesador.\")\n",
    "    \n",
    "    def load_chunk_from_file(self, chunk_id: int) -> Optional[Chunk]:\n",
    "        \"\"\"\n",
    "        Cargar un chunk específico desde archivo\n",
    "        \"\"\"\n",
    "        filename = os.path.join(self.output_folder, f\"chunk_{chunk_id}.pkl\")\n",
    "        \n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"Archivo no encontrado: {filename}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                chunk_data = pickle.load(f)\n",
    "            \n",
    "            chunk = Chunk.from_dict(chunk_data)\n",
    "            print(f\"Chunk {chunk_id} cargado desde archivo\")\n",
    "            return chunk\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar chunk {chunk_id}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_chunk_user_count(self, chunk_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Retornar el número de usuarios de un chunk específico\n",
    "        \"\"\"\n",
    "        if chunk_id < 0 or chunk_id >= self.num_chunks:\n",
    "            print(f\"ID de chunk inválido: {chunk_id}. Debe estar entre 0 y {self.num_chunks-1}\")\n",
    "            return 0\n",
    "        \n",
    "        # Verificar si el chunk está en memoria\n",
    "        if chunk_id < len(self.chunks) and self.chunks[chunk_id].users:\n",
    "            return len(self.chunks[chunk_id].users)\n",
    "        \n",
    "        # Cargar desde archivo\n",
    "        chunk = self.load_chunk_from_file(chunk_id)\n",
    "        if chunk:\n",
    "            return chunk.get_user_count()\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def get_all_chunks_stats(self) -> dict:\n",
    "        \"\"\"\n",
    "        Obtener estadísticas de todos los chunks\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for chunk_id in range(self.num_chunks):\n",
    "            user_count = self.get_chunk_user_count(chunk_id)\n",
    "            stats[f\"chunk_{chunk_id}\"] = {\n",
    "                'user_count': user_count,\n",
    "                'file_exists': os.path.exists(os.path.join(self.output_folder, f\"chunk_{chunk_id}.pkl\"))\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def list_chunk_files(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Listar todos los archivos de chunks disponibles\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        for i in range(self.num_chunks):\n",
    "            filename = os.path.join(self.output_folder, f\"chunk_{i}.pkl\")\n",
    "            if os.path.exists(filename):\n",
    "                files.append(filename)\n",
    "        return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029aee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MovieRatingsProcessor(\n",
    "    threshold=0.003,\n",
    "    num_chunks=4,\n",
    "    output_folder=\"chunks_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e61bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.load_from_csv('ratings_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEstadísticas de chunks:\")\n",
    "stats = processor.get_all_chunks_stats()\n",
    "for chunk_name, chunk_stats in stats.items():\n",
    "    print(f\"  {chunk_name}: {chunk_stats['user_count']} usuarios\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
